# Image classification using Deep Learning models & Transfert Learning

- CNN
- EfficientNet
- InceptionNet
- MobileNetV2
- VGG16
  
# Apply Adversial Attacks
Adversarial examples are specialized inputs created with the aim of confusing a neural network, resulting in the misclassification of a given input. These notorious inputs are indistinguishable to the human eye, but prevent the network from identifying the image content. There are several types of attack of this type, however, the focus here is on the fast gradient sign attack, which is a white-box attack whose aim is to ensure misclassification. A white-box attack is where the attacker has complete access to the figure under attack. One of the most famous examples of a contradictory image shown below is taken from the above-mentioned article.
![image](https://github.com/EL-MEHDI-git/image-processing-project/assets/66147690/de6af3a8-6b58-41b4-9553-a0f06937b90c)

